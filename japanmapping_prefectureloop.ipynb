{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise (don't need maybe?)\n",
    "ee.Initialize()\n",
    "# Configure the pretty printing output & initialize earthengine\n",
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters (INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "maps_outdir = os.path.join(os.path.expanduser('~'), '../Japan_mapping/results/maps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533123b913b5416dacaa36ac98e87951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38, 138], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(children=(To…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map = geemap.Map(center=[38,138], zoom=6)\n",
    "Map.add_basemap('HYBRID')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prefectures shapefile and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan shp is not small, so we ingest it from personal account (otherwise we can upload locally with geemap.shp_to_ee)\n",
    "japan_shp = 'users/luiscartor/japan_gadm0'\n",
    "japan = geemap.ee.FeatureCollection(japan_shp)\n",
    "\n",
    "Map.addLayer(japan, {}, 'Japan')\n",
    "\n",
    "point = ee.Geometry.Point([38,138])\n",
    "Map.addLayer(point, {}, 'Point')\n",
    "\n",
    "# Load prefecture shapefile (from GEE account)\n",
    "prefs_shp = 'users/luiscartor/japan_gadm1'\n",
    "prefs = geemap.ee.FeatureCollection(prefs_shp)\n",
    "\n",
    "Map.addLayer(prefs, {}, 'Prefectures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add year attribute to image\n",
    "def addyear(img):\n",
    "    return img.set('year', ee.Image(img).date().get('year'))\n",
    "\n",
    "# Function to add Day Of Year attribute to image\n",
    "def addDOY(img):\n",
    "    return img.set('DOY', ee.Image(img).date().getRelative('day', 'year'))\n",
    "\n",
    "# Function to full date\n",
    "def addDATE(img):\n",
    "    return ee.Image.constant(img.date().getRelative('day', 'year')).int().updateMask(img.select(0).mask())\n",
    "\n",
    "\n",
    "# Parameters for masking function (move to preamble)\n",
    "def landsatmasking(img):\n",
    "\n",
    "  qa = img.select('pixel_qa')\n",
    "  # If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "  # or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "  cloudorsnow = qa.bitwiseAnd(1 << 5). \\\n",
    "    And(qa.bitwiseAnd(1 << 7)). \\\n",
    "    Or(qa.bitwiseAnd(1 << 3)). \\\n",
    "    Or(qa.bitwiseAnd(1 << 4))  \n",
    "  # Remove edge pixels that don't occur in all bands\n",
    "  mask2 = img.mask().reduce(ee.Reducer.min())\n",
    "  return img.updateMask(cloudorsnow.Not()).updateMask(mask2)\n",
    "\n",
    "# Spectral indices function: adds index as a new band to every image\n",
    "def addINDICES(img):\n",
    "    \n",
    "    ndvi = img.normalizedDifference(['B4', 'B3']).rename('NDVI')\n",
    "    lswi = img.normalizedDifference(['B4', 'B5']).rename('LSWI')\n",
    "    ndsi = img.normalizedDifference(['B2', 'B5']).rename('NDSI')    \n",
    "    # I multiply by scale factor for EVI (DIDNT WORK TO SOLVE HIGH VALUES ISSUE)\n",
    "    evi = img.expression(\n",
    "    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n",
    "        'NIR': img.select('B4').multiply(0.0001),\n",
    "      'RED': img.select('B3').multiply(0.0001),\n",
    "      'BLUE': img.select('B1').multiply(0.0001)\n",
    "    }).rename('EVI')\n",
    "    \n",
    "    return img.addBands(ndvi).addBands(lswi).addBands(ndsi).addBands(evi)\n",
    "\n",
    "# Function to add a band of rice classification to each image in the col\n",
    "# Rule for rice is that LSWI − NDVI > 0 or LSWI − EVI > 0\n",
    "def addRICE(img):    \n",
    "    rice = img.expression(\n",
    "    \"(lswi - ndvi > 0) || (lswi - evi > 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI'),'ndvi': img.select('NDVI'),'evi': img.select('EVI')}).rename('RICE')\n",
    "    \n",
    "    return img.addBands(rice)\n",
    "\n",
    "# Apply Masks Function\n",
    "def landcovermasking(img):\n",
    "  return img.updateMask(mask1.Not()).updateMask(mask2.Not()).updateMask(mask3.Not()).updateMask(mask4.Not())\n",
    "\n",
    "# Function to create binary layer for rice class\n",
    "def classornot(img):\n",
    "    return img.eq(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>8589</th>\n",
       "      <th>9094</th>\n",
       "      <th>9599</th>\n",
       "      <th>0004</th>\n",
       "      <th>0509</th>\n",
       "      <th>1014</th>\n",
       "      <th>1519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hokkaido</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>148</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aomori</td>\n",
       "      <td>139</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "      <td>140</td>\n",
       "      <td>142</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iwate</td>\n",
       "      <td>134</td>\n",
       "      <td>136</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miyagi</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>130</td>\n",
       "      <td>133</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Akita</td>\n",
       "      <td>139</td>\n",
       "      <td>139</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yamagata</td>\n",
       "      <td>136</td>\n",
       "      <td>137</td>\n",
       "      <td>136</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>140</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ibaraki</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tochigi</td>\n",
       "      <td>136</td>\n",
       "      <td>132</td>\n",
       "      <td>129</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gunma</td>\n",
       "      <td>171</td>\n",
       "      <td>169</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture  8589  9094  9599  0004  0509  1014  1519\n",
       "0   Hokkaido   144   144   144   144   144   148   144\n",
       "1     Aomori   139   138   138   137   140   142   141\n",
       "2      Iwate   134   136   135   135   136   138   137\n",
       "3     Miyagi   127   128   126   126   130   133   131\n",
       "4      Akita   139   139   138   138   139   143   142\n",
       "5   Yamagata   136   137   136   137   137   140   138\n",
       "6  Fukushima   135   136   134   134   134   138   137\n",
       "7    Ibaraki   127   127   126   125   125   127   126\n",
       "8    Tochigi   136   132   129   127   127   127   127\n",
       "9      Gunma   171   169   168   164   164   166   164"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of prefectures with flooding data\n",
    "transplantdates_table = '../Japan_mapping/data/transplantdates_average.csv'\n",
    "transplantdates = pd.read_csv(transplantdates_table, sep=\",\")\n",
    "\n",
    "transplantdates.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>Meteostation</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hokkaido</td>\n",
       "      <td>Sapporo</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>-5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aomori</td>\n",
       "      <td>Aomori</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iwate</td>\n",
       "      <td>Morioka</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miyagi</td>\n",
       "      <td>Sendai</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Akita</td>\n",
       "      <td>Akita</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yamagata</td>\n",
       "      <td>Yamagata</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>Fukushima</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ibaraki</td>\n",
       "      <td>Mito</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tochigi</td>\n",
       "      <td>Utsunomiya</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gunma</td>\n",
       "      <td>Maebashi</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture Meteostation    1    2    3    4    5    6    7    8  ...  356  \\\n",
       "0   Hokkaido      Sapporo -5.8 -5.9 -6.0 -6.1 -6.3 -6.4 -6.5 -6.6  ... -5.0   \n",
       "1     Aomori       Aomori -2.8 -2.9 -3.0 -3.1 -3.2 -3.3 -3.3 -3.4  ... -2.1   \n",
       "2      Iwate      Morioka -4.2 -4.4 -4.5 -4.6 -4.7 -4.9 -5.0 -5.1  ... -3.2   \n",
       "3     Miyagi       Sendai -0.7 -0.8 -0.9 -1.0 -1.1 -1.2 -1.2 -1.3  ...  0.2   \n",
       "4      Akita        Akita -1.4 -1.5 -1.6 -1.7 -1.8 -1.9 -2.0 -2.1  ... -0.7   \n",
       "5   Yamagata     Yamagata -2.3 -2.4 -2.5 -2.6 -2.7 -2.8 -2.9 -3.0  ... -1.4   \n",
       "6  Fukushima    Fukushima -0.9 -1.0 -1.1 -1.2 -1.2 -1.3 -1.4 -1.4  ...  0.0   \n",
       "7    Ibaraki         Mito -1.6 -1.7 -1.8 -1.8 -1.8 -1.9 -1.9 -2.0  ... -0.7   \n",
       "8    Tochigi   Utsunomiya -2.1 -2.2 -2.2 -2.3 -2.3 -2.4 -2.4 -2.5  ... -1.3   \n",
       "9      Gunma     Maebashi -0.1 -0.1 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5  ...  0.9   \n",
       "\n",
       "   357  358  359  360  361  362  363  364  365  \n",
       "0 -5.0 -5.1 -5.1 -5.2 -5.3 -5.4 -5.5 -5.6 -5.7  \n",
       "1 -2.2 -2.2 -2.3 -2.3 -2.4 -2.5 -2.5 -2.6 -2.7  \n",
       "2 -3.3 -3.4 -3.4 -3.5 -3.6 -3.7 -3.8 -4.0 -4.1  \n",
       "3  0.2  0.1  0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6  \n",
       "4 -0.7 -0.8 -0.8 -0.9 -1.0 -1.0 -1.1 -1.2 -1.3  \n",
       "5 -1.5 -1.5 -1.6 -1.7 -1.8 -1.9 -2.0 -2.1 -2.2  \n",
       "6 -0.1 -0.2 -0.3 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8  \n",
       "7 -0.9 -1.0 -1.1 -1.1 -1.2 -1.3 -1.4 -1.5 -1.6  \n",
       "8 -1.4 -1.5 -1.6 -1.7 -1.7 -1.8 -1.9 -2.0 -2.1  \n",
       "9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.0  \n",
       "\n",
       "[10 rows x 367 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of prefectures with DOY per temperature data\n",
    "# temperature_table = '../Japan_mapping/data/prefecturetempdata.csv'\n",
    "# tempdata = pd.read_csv(temperature_table, sep=\",\")\n",
    "\n",
    "temperature_table = '../Japan_mapping/data/prefectureDOYmintemp.csv'\n",
    "tempdata = pd.read_csv(temperature_table, sep=\",\")\n",
    "\n",
    "tempdata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table with DOY with first/last 0/5/10 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>TGS0S</th>\n",
       "      <th>TGS0E</th>\n",
       "      <th>TGS5S</th>\n",
       "      <th>TGS5E</th>\n",
       "      <th>TGS10S</th>\n",
       "      <th>TGS10E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hokkaido</td>\n",
       "      <td>90</td>\n",
       "      <td>327</td>\n",
       "      <td>115</td>\n",
       "      <td>302</td>\n",
       "      <td>148</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aomori</td>\n",
       "      <td>85</td>\n",
       "      <td>339</td>\n",
       "      <td>113</td>\n",
       "      <td>310</td>\n",
       "      <td>144</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iwate</td>\n",
       "      <td>90</td>\n",
       "      <td>329</td>\n",
       "      <td>116</td>\n",
       "      <td>299</td>\n",
       "      <td>146</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miyagi</td>\n",
       "      <td>68</td>\n",
       "      <td>360</td>\n",
       "      <td>99</td>\n",
       "      <td>321</td>\n",
       "      <td>128</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Akita</td>\n",
       "      <td>75</td>\n",
       "      <td>349</td>\n",
       "      <td>104</td>\n",
       "      <td>316</td>\n",
       "      <td>133</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yamagata</td>\n",
       "      <td>81</td>\n",
       "      <td>344</td>\n",
       "      <td>108</td>\n",
       "      <td>311</td>\n",
       "      <td>135</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>68</td>\n",
       "      <td>357</td>\n",
       "      <td>99</td>\n",
       "      <td>319</td>\n",
       "      <td>125</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ibaraki</td>\n",
       "      <td>62</td>\n",
       "      <td>351</td>\n",
       "      <td>96</td>\n",
       "      <td>322</td>\n",
       "      <td>123</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tochigi</td>\n",
       "      <td>65</td>\n",
       "      <td>348</td>\n",
       "      <td>95</td>\n",
       "      <td>320</td>\n",
       "      <td>120</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gunma</td>\n",
       "      <td>53</td>\n",
       "      <td>365</td>\n",
       "      <td>91</td>\n",
       "      <td>327</td>\n",
       "      <td>116</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture  TGS0S  TGS0E  TGS5S  TGS5E  TGS10S  TGS10E\n",
       "0   Hokkaido     90    327    115    302     148     278\n",
       "1     Aomori     85    339    113    310     144     283\n",
       "2      Iwate     90    329    116    299     146     278\n",
       "3     Miyagi     68    360     99    321     128     295\n",
       "4      Akita     75    349    104    316     133     289\n",
       "5   Yamagata     81    344    108    311     135     287\n",
       "6  Fukushima     68    357     99    319     125     293\n",
       "7    Ibaraki     62    351     96    322     123     297\n",
       "8    Tochigi     65    348     95    320     120     296\n",
       "9      Gunma     53    365     91    327     116     301"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectors to store TGS values (first day of certain min temperature)\n",
    "TGS0S = []; TGS0E = []; TGS5S = []; TGS5E = []; TGS10S = []; TGS10E = [];\n",
    "\n",
    "\n",
    "# We loop through each prefecture\n",
    "for i in range(len(tempdata)):\n",
    "    #print(i)\n",
    "#i = 0\n",
    "\n",
    "    # Extracts first DOY with mintemp > 0,5,10\n",
    "    T0S = next(x for x, val in enumerate(tempdata.iloc[i,2:]) if val > 0)\n",
    "    T5S = next(x for x, val in enumerate(tempdata.iloc[i,2:]) if val > 5)\n",
    "    T10S = next(x for x, val in enumerate(tempdata.iloc[i,2:]) if val > 10)\n",
    "\n",
    "    # Extracts first DOY with mintemp < 0,5,10 after summer (we consider temp values since August (DOY=214))\n",
    "    # If value does not exist (desired temp falls into next year), we set to last day of year (so to not to use following year image)\n",
    "    T0E = next((x for x, val in enumerate(tempdata.iloc[i,215:]) if val < 0), 365-214) + 214\n",
    "    T5E = next((x for x, val in enumerate(tempdata.iloc[i,215:]) if val < 5), 365-214) + 214\n",
    "    T10E = next((x for x, val in enumerate(tempdata.iloc[i,215:]) if val < 10), 365-214) + 214\n",
    "\n",
    "    TGS0S.append(T0S); TGS5S.append(T5S); TGS10S.append(T10S); TGS0E.append(T0E); TGS5E.append(T5E); TGS10E.append(T10E); \n",
    "\n",
    "tgstable = pd.DataFrame({'Prefecture': tempdata['Prefecture'], 'TGS0S': TGS0S, 'TGS0E': TGS0E, 'TGS5S': TGS5S\n",
    "                        , 'TGS5E': TGS5E, 'TGS10S': TGS10S, 'TGS10E': TGS10E})\n",
    "tgstable.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataframes and lists used in loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame for storing image availability\n",
    "availabilitytable = pd.DataFrame([])\n",
    "\n",
    "\n",
    "# Define periods names: dont' edit (used in dictionary loop later)\n",
    "periodnames = {'8590','9094','9599','0004','0509','1014','1520'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landsat 5\n",
    "L5col = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR') \\\n",
    "    .select('B[1-7]','pixel_qa') \\\n",
    "    .map(addyear) \\\n",
    "    .map(addDOY)\n",
    "       \n",
    "# Landsat 7\n",
    "L7col = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR') \\\n",
    "    .select('B[1-7]','pixel_qa') \\\n",
    "    .map(addyear) \\\n",
    "    .map(addDOY)\n",
    "\n",
    "# Merge collections\n",
    "L57col = L5col.merge(L7col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Year period collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>8589</th>\n",
       "      <th>9094</th>\n",
       "      <th>9599</th>\n",
       "      <th>0004</th>\n",
       "      <th>0509</th>\n",
       "      <th>1014</th>\n",
       "      <th>1519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hokkaido</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>148</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aomori</td>\n",
       "      <td>139</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "      <td>140</td>\n",
       "      <td>142</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture  8589  9094  9599  0004  0509  1014  1519\n",
       "0   Hokkaido   144   144   144   144   144   148   144\n",
       "1     Aomori   139   138   138   137   140   142   141"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset to 5-year periods\n",
    "L57_8589 = L57col.filterDate('1985-01-01', '1989-12-31')\n",
    "L57_9094 = L57col.filterDate('1990-01-01', '1994-12-31')\n",
    "L57_9599 = L57col.filterDate('1995-01-01', '1999-12-31')\n",
    "L57_0004 = L57col.filterDate('2000-01-01', '2004-12-31')\n",
    "L57_0509 = L57col.filterDate('2005-01-01', '2009-12-31')\n",
    "L57_1014 = L57col.filterDate('2010-01-01', '2014-12-31')\n",
    "L57_1519 = L57col.filterDate('2015-01-01', '2019-12-31')\n",
    "\n",
    "# Dictionary with periods and collections\n",
    "periods_dict = {'8589': L57_8589,\n",
    "              '9094': L57_9094,\n",
    "              '9599': L57_9599,\n",
    "              '0004': L57_0004,\n",
    "              '0509': L57_0509,\n",
    "              '1014': L57_1014,\n",
    "              '1519': L57_1519}\n",
    "\n",
    "transplantdates = transplantdates.iloc[:2]\n",
    "transplantdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting ricemap_8589 ...\n",
      "Exporting ricemap_9094 ...\n",
      "Exporting ricemap_9599 ...\n"
     ]
    }
   ],
   "source": [
    "# List for storing national rice maps for each period\n",
    "periodmapslist = list()\n",
    "\n",
    "# LOOP FOR PERIODS\n",
    "for periodcol in periods_dict.values():\n",
    "    #periodcol = L57_8589\n",
    "\n",
    "    # List for store prefectural maps for current loop period\n",
    "    prefmapslist = list()\n",
    "    \n",
    "    # LOOP FOR PREFECTURES\n",
    "    for prefecture in transplantdates['Prefecture']:\n",
    "        \n",
    "        # Extract prefecture polygon\n",
    "        prefpolygon = prefs \\\n",
    "            .filter(ee.Filter.eq(\"NAME_1\", prefecture)) \n",
    "        # Clip collection to prefecture\n",
    "        prefcol = periodcol.filterBounds(prefpolygon)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # PRE-PROCESSING\n",
    "        prefcol_masked = prefcol.map(landsatmasking)\n",
    "        # Add Indices\n",
    "        prefcol_indices = prefcol_masked.map(addINDICES)\n",
    "        # Subset to spectral indices\n",
    "        prefcol_ind = prefcol_indices.select('NDVI','LSWI','NDSI','EVI')\n",
    "        \n",
    "        \n",
    "#         # IMAGE AVAILABILITY PER PERIOD\n",
    "#         # Obtain number of images per year; avoid same date pixels (different tile but same date), we count disctinct dates\n",
    "#         freq = prefcol_ind.map(addDATE).reduce(ee.Reducer.countDistinct())\n",
    "\n",
    "#         # Visualize map\n",
    "#         #Map2.addLayer(freq, {'bands': ['constant_count'],'min':0,'max':50,'palette': ['00FFFF', '0000FF']}, \n",
    "#         #             'Im. Frequency period: ' + list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)])\n",
    "\n",
    "#         # Histogram of number of images per pixel\n",
    "#         histogram = freq.reduceRegion(**{\n",
    "#         'reducer': ee.Reducer.histogram(),\n",
    "#         'geometry': prefpolygon.geometry(),\n",
    "#         'scale': 30,\n",
    "#         'maxPixels': 1e9\n",
    "#         })\n",
    "\n",
    "#         # Convert histogram (dictionary type) into list\n",
    "#         hist = histogram.getInfo()\n",
    "#         hist_list = list(hist.values())\n",
    "\n",
    "#         # Plot histogram\n",
    "#         plt.figure()  # This is needed to plot multiple figures in plot\n",
    "#         plt.bar(hist_list[0]['bucketMeans'], hist_list[0]['histogram'], color='g')\n",
    "#         plt.ylabel('Number of pixels')\n",
    "#         plt.xlabel('Number of images')\n",
    "#         plt.title('# of images per pixel for the period '+ \n",
    "#                list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "#                 ' for '+ prefecture)\n",
    "#         # Save figure\n",
    "#         plt.savefig('../Japan_mapping/results/figures/imgavailability_period'+\n",
    "#                  list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "#                  prefecture +'_hist.eps', bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "        # LUIS OF TOMORROW: ADD PERIODS TO FLOODING DATES!!!! AND ADAPT SCRIPT!!\n",
    "        # EXTRACT FLOODING PERIODS AND ESTIMATE IMAGE AVAILABILITY\n",
    "        # Flooding start day is considered the first day of transplanting (TS)\n",
    "        flood_start = int(transplantdates[list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]][transplantdates[transplantdates['Prefecture']==prefecture].index[0]])\n",
    "        # MAKE A DIFFERENT TABLE FOR END PERIOD?\n",
    "        # Flooding last day is considered the end day of transplanting + 30 days !!!!CHECK THE END OF TRANSPLANTING!!!\n",
    "        flood_end = int(transplantdates[list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]][transplantdates[transplantdates['Prefecture']==prefecture].index[0]] + 30)\n",
    "      \n",
    "        # List for each period (year period and irrigation period)\n",
    "        # Subset to flooding period\n",
    "        prefcol_flood = prefcol_ind.filter(ee.Filter.gte('DOY', flood_start)).filter(ee.Filter.lte('DOY', flood_end))\n",
    "\n",
    "        # Subset to non-flooding period\n",
    "        prefcol_noflood1 = prefcol_ind.filter(ee.Filter.lte('DOY', flood_start))\n",
    "        prefcol_noflood2 = prefcol_ind.filter(ee.Filter.gte('DOY', flood_end))\n",
    "        prefcol_noflood = prefcol_noflood1.merge(prefcol_noflood2)        \n",
    "        \n",
    "        # Add number of images per season and period into table\n",
    "        d = {'Prefecture': [prefecture], \n",
    "             'Lustrum': [list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]], \n",
    "             'AvImFlooding': [prefcol_flood.size().getInfo()], \n",
    "             'AvImNonFlooding': [prefcol_noflood.size().getInfo()]} \n",
    "        availabilitytable_pref = pd.DataFrame(data=d)\n",
    "\n",
    "        # Add prefecture to table\n",
    "        availabilitytable = availabilitytable.append(availabilitytable_pref)\n",
    "\n",
    "        #print(availabilitytable_pref)\n",
    "        #print(availabilitytable)\n",
    "\n",
    "#         # Histogram of availability during flooding period\n",
    "#         freq = prefcol_flood.reduce(ee.Reducer.countDistinct())\n",
    "#         # Histogram of number of images per pixel\n",
    "#         histogram = freq.reduceRegion(**{\n",
    "#           'reducer': ee.Reducer.histogram(),\n",
    "#           'geometry': prefpolygon.geometry(),\n",
    "#           'scale': 30,\n",
    "#           'maxPixels': 1e9\n",
    "#         })\n",
    "\n",
    "#         # Convert histogram (dictionary type) into list\n",
    "#         hist = histogram.getInfo()\n",
    "#         hist_list = list(hist.values())\n",
    "\n",
    "#         # Plot histogram\n",
    "#         plt.figure()  # This is needed to plot multiple figures in plot\n",
    "#         plt.bar(hist_list[0]['bucketMeans'], hist_list[0]['histogram'], color='g')\n",
    "#         plt.ylabel('Number of pixels')\n",
    "#         plt.xlabel('Number of images')\n",
    "#         plt.title('# of images per pixel for flooding period during '+ list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "#                  ' for ' + prefecture)\n",
    "#         # Save figure\n",
    "#         plt.savefig('../Japan_mapping/results/figures/floodimageavailability_period'+\n",
    "#                     list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "#                     '_'+prefecture+'_hist.eps', bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "        # FILTER COLLECTIONS FOR TEMPERATURE PERIODS\n",
    "        # Extracting DOY for temperature data\n",
    "        TGS0S = int(tgstable['TGS0S'][tgstable[tgstable['Prefecture']==prefecture].index[0]])\n",
    "        TGS0E = int(tgstable['TGS0E'][tgstable[tgstable['Prefecture']==prefecture].index[0]]) \n",
    "        TGS5S = int(tgstable['TGS5S'][tgstable[tgstable['Prefecture']==prefecture].index[0]]) \n",
    "        TGS5E = int(tgstable['TGS5E'][tgstable[tgstable['Prefecture']==prefecture].index[0]]) \n",
    "        TGS10S = int(tgstable['TGS10S'][tgstable[tgstable['Prefecture']==prefecture].index[0]]) \n",
    "        TGS10E = int(tgstable['TGS10E'][tgstable[tgstable['Prefecture']==prefecture].index[0]])\n",
    "        \n",
    "        \n",
    "        # Subset to period between first 0 and last 0 min degrees:\n",
    "        L57_0S0E = prefcol_ind.filter(ee.Filter.gte('DOY', TGS0S)).filter(ee.Filter.lte('DOY', TGS0E))\n",
    "        # Subset to period between first 5 and last 5 min degrees:\n",
    "        L57_5S5E = prefcol_ind.filter(ee.Filter.gte('DOY', TGS5S)).filter(ee.Filter.lte('DOY', TGS5E))\n",
    "        # Subset to period between first 0 and first 10 min degrees:\n",
    "        L57_0S10S = prefcol_ind.filter(ee.Filter.gte('DOY', TGS0S)).filter(ee.Filter.lte('DOY', TGS10S))\n",
    "        # Subset to period between first 10 + 40days and last 10 min degrees:\n",
    "        L57_10S40to5E = prefcol_ind.filter(ee.Filter.gte('DOY', TGS10S+40)).filter(ee.Filter.lte('DOY', TGS10E))\n",
    "        # Subset to period between last 10 and first 10 (not thermal growing season)\n",
    "        L57_10E10S1 = prefcol_ind.filter(ee.Filter.lte('DOY', TGS10S))\n",
    "        L57_10E10S2 = prefcol_ind.filter(ee.Filter.gte('DOY', TGS10E))\n",
    "        L57_10E10S = L57_10E10S1.merge(L57_10E10S2)\n",
    "\n",
    "        # Calculate median value for indeces for rice flooding period\n",
    "        L57_flood_median = prefcol_flood.median()\n",
    "\n",
    "        \n",
    "        # APPLY MASKS (based on Dong 2016, RSE)\n",
    "\n",
    "        # Mask 1) Sparce vegetation: soil, built-up, water body, low vegetated lands. max EVI (T5S-T5E) < 0.6\n",
    "        # I could use some percentile, instead of max, in order to avoid outliers\n",
    "        L57_5S5E_median = L57_5S5E.median()\n",
    "        L57_5S5E_max = L57_5S5E.max()\n",
    "\n",
    "        mask1 = L57_5S5E_max.select('EVI').lt(0.5)\n",
    "\n",
    "        # Mask 2) Natural vegetation mask: forests, natural wetlands, grass. Max EVI (T10E-T10S) > 0.4\n",
    "        L57_10E10S_median = L57_10E10S.median()\n",
    "        L57_10E10S_max = L57_10E10S.max()\n",
    "\n",
    "        mask2 = L57_10E10S_median.select('EVI').gt(0.2)\n",
    "\n",
    "        #SOLVE ISSUE: EVI GIVING VALUES ABOVE 2!\n",
    "\n",
    "        # Mask 3) Forest mask (JAXA)\n",
    "        # Starts from 2007\n",
    "        forestdataset = ee.ImageCollection('JAXA/ALOS/PALSAR/YEARLY/FNF') \\\n",
    "            .filterDate('2017-01-01', '2017-12-31');\n",
    "\n",
    "        # Forest is the fnf = 1\n",
    "        forest = forestdataset.first().select('fnf').eq(1);\n",
    "        # Create forest mask\n",
    "        mask3 = forest\n",
    "\n",
    "        # MASK 3 NOT WORKING VERY WELL. ALOS MAP GETS A LOT OF FALSE POSITIVE FOREST (REDUCING RICE FIELDS)\n",
    "\n",
    "        # Mask 4) Slope: larger than 3deg\n",
    "        # Call SRTM elevation dataset\n",
    "        elevdataset = ee.Image('JAXA/ALOS/AW3D30/V2_2');\n",
    "        elevation = elevdataset.select('AVE_DSM');\n",
    "        # Obtain \n",
    "        slope = ee.Terrain.slope(elevation);\n",
    "\n",
    "        # Create mask ro slope > 3 deg\n",
    "        mask4 = slope.gt(3)\n",
    "        \n",
    "        # APPLY MASKS    \n",
    "        prefcol_flood_masked =  prefcol_flood.map(landcovermasking)\n",
    "\n",
    "        \n",
    "        # RICE CLASSIFICATION\n",
    "        # Method 2 for classifying based on % of images\n",
    "        \n",
    "        # Add band to each image for rice classification\n",
    "        prefcol_flood_masked =  prefcol_flood.map(landcovermasking)\n",
    "\n",
    "        # Add rice classification layer\n",
    "        prefcol_ricecol = prefcol_flood_masked.map(addRICE).select('RICE')\n",
    "\n",
    "        # Now we apply the % of images threshold to classify as rice\n",
    "        # Reduce the number of images with classornot pixels\n",
    "        riceprop = prefcol_ricecol.map(classornot).mean()\n",
    "\n",
    "        # Select % of image necessary to assing rice!\n",
    "        riceclass = riceprop.gte(0.1).clip(prefpolygon)\n",
    "\n",
    "        # Insert map in list\n",
    "        prefmapslist.append(riceclass)\n",
    "\n",
    "        # End of prefectures loop\n",
    "        \n",
    "    # Mosaic all prefectural maps for current loop period\n",
    "    periodricecol = ee.ImageCollection.fromImages(prefmapslist)\n",
    "    periodmosaic = periodricecol.mosaic()\n",
    "        \n",
    "    # Store period mosaic in list\n",
    "    periodmapslist.append(periodmosaic)\n",
    "    \n",
    "#     # Save period national mosaic to disk: TOO LARGE FOR ALL JAPAN!\n",
    "#     filename = os.path.join(maps_outdir, 'ricemap_'+\n",
    "#                            list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+'.tif')\n",
    "#     geemap.ee_export_image(periodmosaic, filename=filename, region=japan.geometry(), scale=30, file_per_band=False)\n",
    "    \n",
    "    # Save period national mosaic to drive: CHANGE REGION TO \"japan\"\n",
    "    filename = 'ricemap_'+list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]\n",
    "    geemap.ee_export_image_to_drive(periodmosaic, description=filename, folder='GEEexports', region=prefpolygon.geometry(), scale=30)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(availabilitytable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map2 = geemap.Map(center=[38,138], zoom=6)\n",
    "Map2.add_basemap('HYBRID')\n",
    "Map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(periodmapslist))\n",
    "prefcol = ee.ImageCollection.fromImages(prefmapslist)\n",
    "#print(prefcol.size().getInfo())\n",
    "\n",
    "Map2.addLayer(prefcol, {}, 'lastperiod')\n",
    "\n",
    "Map2.addLayer(periodmapslist[0], {}, 'period1')\n",
    "Map2.addLayer(periodmapslist[1], {}, 'period2')\n",
    "Map2.addLayer(periodmapslist[2], {}, 'period3')\n",
    "Map2.addLayer(periodmapslist[3], {}, 'period4')\n",
    "Map2.addLayer(periodmapslist[4], {}, 'period5')\n",
    "Map2.addLayer(periodmapslist[5], {}, 'period6')\n",
    "Map2.addLayer(periodmapslist[6], {}, 'period7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846f6c4049fd47dba6f1a59431e8e2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38, 138], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleButton(value=F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map3 = geemap.Map(center=[38,138], zoom=7)\n",
    "right_layer = geemap.ee_tile_layer(periodmapslist[0], {}, 'Rice 1985-90')\n",
    "left_layer = geemap.ee_tile_layer(periodmapslist[6], {}, 'Rice 2015-2020')\n",
    "#Map3.split_map(left_layer, right_layer)\n",
    "Map3.add_basemap('HYBRID')\n",
    "Map3.split_map(left_layer='HYBRID', right_layer=right_layer)\n",
    "Map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af699402a4402db80c3a18983a6cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38, 138], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleButton(value=F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map4 = geemap.Map(center=[38,138], zoom=7)\n",
    "right_layer = geemap.ee_tile_layer(periodmapslist[0], {}, 'Rice 1985-90')\n",
    "left_layer = geemap.ee_tile_layer(periodmapslist[6], {}, 'Rice 2015-2020')\n",
    "Map4.split_map(left_layer, right_layer)\n",
    "Map4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPERVIOUS AND BARREN\n",
    "# Rule for impervious and barren is LSWI < 0 \n",
    "def addIMPERV(img):    \n",
    "    imperv = img.expression(\n",
    "    \"(lswi < 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI')}).rename('IMPERV')\n",
    "    \n",
    "    return img.addBands(imperv)\n",
    "\n",
    "L57_8589_impervcol = L57_5S5E.map(addIMPERV).select('IMPERV')\n",
    "\n",
    "# Now we apply the % of images threshold to classify as rice\n",
    "def classornot(img):\n",
    "    return img.eq(1)\n",
    "\n",
    "# Reduce the number of images with classornot pixels\n",
    "impervprop = L57_8589_impervcol.map(classornot).mean()\n",
    "# Select % of image necessary to assing imperv\n",
    "impervclass = impervprop.gte(0.5)\n",
    "\n",
    "# EVERGREEN\n",
    "# Rule for evergreen is LSWI > 0 \n",
    "def addEVER(img):    \n",
    "    ever = img.expression(\n",
    "    \"(lswi > 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI')}).rename('EVER')\n",
    "    \n",
    "    return img.addBands(ever)\n",
    "\n",
    "L57_8589_evercol = L57_8589.map(addEVER).select('EVER')\n",
    "\n",
    "# Now we apply the % of images threshold to classify ever\n",
    "def classornot(img):\n",
    "    return img.eq(1)\n",
    "\n",
    "# Reduce the number of images with classornot pixels\n",
    "everprop = L57_8589_evercol.map(classornot).mean()\n",
    "# Select % of image necessary to assing imperv\n",
    "everclass = everprop.gte(0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
