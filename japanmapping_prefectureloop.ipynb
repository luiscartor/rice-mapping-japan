{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise (don't need maybe?)\n",
    "ee.Initialize()\n",
    "# Configure the pretty printing output & initialize earthengine\n",
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters (INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1073b093971f436c82ddbd9e08c39e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38, 138], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleButton(value=F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map = geemap.Map(center=[38,138], zoom=6)\n",
    "Map.add_basemap('HYBRID')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prefectures shapefile and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan shp is not small, so we ingest it from personal account (otherwise we can upload locally with geemap.shp_to_ee)\n",
    "japan_shp = 'users/luiscartor/japan_gadm0'\n",
    "japan = geemap.ee.FeatureCollection(japan_shp)\n",
    "\n",
    "Map.addLayer(japan, {}, 'Japan')\n",
    "\n",
    "point = ee.Geometry.Point([38,138])\n",
    "Map.addLayer(point, {}, 'Point')\n",
    "\n",
    "# Load prefecture shapefile (from GEE account)\n",
    "prefs_shp = 'users/luiscartor/japan_gadm1'\n",
    "prefs = geemap.ee.FeatureCollection(prefs_shp)\n",
    "\n",
    "Map.addLayer(prefs, {}, 'Prefectures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add year attribute to image\n",
    "def addyear(img):\n",
    "    return img.set('year', ee.Image(img).date().get('year'))\n",
    "\n",
    "# Function to add Day Of Year attribute to image\n",
    "def addDOY(img):\n",
    "    return img.set('DOY', ee.Image(img).date().getRelative('day', 'year'))\n",
    "\n",
    "# Function to full date\n",
    "def addDATE(img):\n",
    "    return ee.Image.constant(img.date().getRelative('day', 'year')).int().updateMask(img.select(0).mask())\n",
    "\n",
    "\n",
    "# Parameters for masking function (move to preamble)\n",
    "def landsatmasking(img):\n",
    "\n",
    "  qa = img.select('pixel_qa')\n",
    "  # If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "  # or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "  cloudorsnow = qa.bitwiseAnd(1 << 5). \\\n",
    "    And(qa.bitwiseAnd(1 << 7)). \\\n",
    "    Or(qa.bitwiseAnd(1 << 3)). \\\n",
    "    Or(qa.bitwiseAnd(1 << 4))  \n",
    "  # Remove edge pixels that don't occur in all bands\n",
    "  mask2 = img.mask().reduce(ee.Reducer.min())\n",
    "  return img.updateMask(cloudorsnow.Not()).updateMask(mask2)\n",
    "\n",
    "# Spectral indices function: adds index as a new band to every image\n",
    "def addINDICES(img):\n",
    "    \n",
    "    ndvi = img.normalizedDifference(['B4', 'B3']).rename('NDVI')\n",
    "    lswi = img.normalizedDifference(['B4', 'B5']).rename('LSWI')\n",
    "    ndsi = img.normalizedDifference(['B2', 'B5']).rename('NDSI')    \n",
    "    # I multiply by scale factor for EVI (DIDNT WORK TO SOLVE HIGH VALUES ISSUE)\n",
    "    evi = img.expression(\n",
    "    '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n",
    "        'NIR': img.select('B4').multiply(0.0001),\n",
    "      'RED': img.select('B3').multiply(0.0001),\n",
    "      'BLUE': img.select('B1').multiply(0.0001)\n",
    "    }).rename('EVI')\n",
    "    \n",
    "    return img.addBands(ndvi).addBands(lswi).addBands(ndsi).addBands(evi)\n",
    "\n",
    "# Function to add a band of rice classification to each image in the col\n",
    "# Rule for rice is that LSWI − NDVI > 0 or LSWI − EVI > 0\n",
    "def addRICE(img):    \n",
    "    rice = img.expression(\n",
    "    \"(lswi - ndvi > 0) || (lswi - evi > 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI'),'ndvi': img.select('NDVI'),'evi': img.select('EVI')}).rename('RICE')\n",
    "    \n",
    "    return img.addBands(rice)\n",
    "\n",
    "# Apply Masks Function\n",
    "def landcovermasking(img):\n",
    "  return img.updateMask(mask1.Not()).updateMask(mask2.Not()).updateMask(mask3.Not()).updateMask(mask4.Not())\n",
    "\n",
    "# Function to create binary layer for rice class\n",
    "def classornot(img):\n",
    "    return img.eq(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ibaraki</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chiba</td>\n",
       "      <td>100</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture  startdate  enddate\n",
       "0    Ibaraki        120      150\n",
       "1      Chiba        100      130"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of prefectures with flooding data\n",
    "prefectures_table = '../Japan_mapping/data/prefecturesdata.csv'\n",
    "prefsdata = pd.read_csv(prefectures_table, sep=\",\")\n",
    "\n",
    "#prefsdata.head(10)\n",
    "prefsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prefecture</th>\n",
       "      <th>Lustrum</th>\n",
       "      <th>TGS-0-S</th>\n",
       "      <th>TGS-0-E</th>\n",
       "      <th>TGS-5-S</th>\n",
       "      <th>TGS-5-E</th>\n",
       "      <th>TGS-10-S</th>\n",
       "      <th>TGS-10-E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ibaraki</td>\n",
       "      <td>8589</td>\n",
       "      <td>70</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>250</td>\n",
       "      <td>120</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chiba</td>\n",
       "      <td>8589</td>\n",
       "      <td>75</td>\n",
       "      <td>295</td>\n",
       "      <td>100</td>\n",
       "      <td>250</td>\n",
       "      <td>120</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prefecture  Lustrum  TGS-0-S  TGS-0-E  TGS-5-S  TGS-5-E  TGS-10-S  TGS-10-E\n",
       "0    Ibaraki     8589       70      300      100      250       120       180\n",
       "1      Chiba     8589       75      295      100      250       120       180"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of prefectures with DOY per temperature data\n",
    "temperature_table = '../Japan_mapping/data/prefecturetempdata.csv'\n",
    "tempdata = pd.read_csv(temperature_table, sep=\",\")\n",
    "\n",
    "#tempdata.head(10)\n",
    "tempdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataframes and lists used in loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame for storing image availability\n",
    "availabilitytable = pd.DataFrame([])\n",
    "\n",
    "# List for storing prefectural rice maps\n",
    "prefmapslist = list()\n",
    "\n",
    "# Define periods names: dont' edit (used in dictionary loop later)\n",
    "periodnames = {'8590','9094','9599','0004','0509','1014','1520'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landsat 5\n",
    "L5col = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR') \\\n",
    "    .select('B[1-7]','pixel_qa') \\\n",
    "    .map(addyear) \\\n",
    "    .map(addDOY)\n",
    "       \n",
    "# Landsat 7\n",
    "L7col = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR') \\\n",
    "    .select('B[1-7]','pixel_qa') \\\n",
    "    .map(addyear) \\\n",
    "    .map(addDOY)\n",
    "\n",
    "# Merge collections\n",
    "L57col = L5col.merge(L7col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Year period collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to 5-year periods\n",
    "L57_8589 = L57col.filterDate('1985-01-01', '1989-12-31')\n",
    "L57_9094 = L57col.filterDate('1990-01-01', '1994-12-31')\n",
    "L57_9599 = L57col.filterDate('1995-01-01', '1999-12-31')\n",
    "L57_0004 = L57col.filterDate('2000-01-01', '2004-12-31')\n",
    "L57_0509 = L57col.filterDate('2005-01-01', '2009-12-31')\n",
    "L57_1014 = L57col.filterDate('2010-01-01', '2014-12-31')\n",
    "L57_1520 = L57col.filterDate('2015-01-01', '2019-12-31')\n",
    "\n",
    "# Dictionary with periods and collections\n",
    "periods_dict = {'8590': L57_8589,\n",
    "              '9094': L57_9094}\n",
    "              #'9599': L57_9599,\n",
    "              #'0004': L57_0004,\n",
    "              #'0509': L57_0509,\n",
    "              #'1014': L57_1014,\n",
    "              #'1520': L57_1520}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP FOR PERIODS\n",
    "for periodcol in periods_dict.values():\n",
    "    #periodcol = L57_8589\n",
    "    \n",
    "    # LOOP FOR PREFECTURES\n",
    "    for prefecture in prefsdata['Prefecture']:\n",
    "        \n",
    "        # Extract prefecture polygon\n",
    "        prefpolygon = prefs \\\n",
    "            .filter(ee.Filter.eq(\"NAME_1\", prefecture)) \n",
    "        # Clip collection to prefecture\n",
    "        prefcol = periodcol.filterBounds(prefpolygon)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # PRE-PROCESSING\n",
    "        prefcol_masked = prefcol.map(landsatmasking)\n",
    "        # Add Indices\n",
    "        prefcol_indices = prefcol_masked.map(addINDICES)\n",
    "        # Subset to spectral indices\n",
    "        prefcol_ind = prefcol_indices.select('NDVI','LSWI','NDSI','EVI')\n",
    "        \n",
    "        \n",
    "        # IMAGE AVAILABILITY PER PERIOD\n",
    "        # Obtain number of images per year; avoid same date pixels (different tile but same date), we count disctinct dates\n",
    "        freq = prefcol_ind.map(addDATE).reduce(ee.Reducer.countDistinct())\n",
    "\n",
    "        # Visualize map\n",
    "        #Map2.addLayer(freq, {'bands': ['constant_count'],'min':0,'max':50,'palette': ['00FFFF', '0000FF']}, \n",
    "        #             'Im. Frequency period: ' + list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)])\n",
    "\n",
    "        # Histogram of number of images per pixel\n",
    "        histogram = freq.reduceRegion(**{\n",
    "        'reducer': ee.Reducer.histogram(),\n",
    "        'geometry': prefpolygon.geometry(),\n",
    "        'scale': 30,\n",
    "        'maxPixels': 1e9\n",
    "        })\n",
    "\n",
    "        # Convert histogram (dictionary type) into list\n",
    "        hist = histogram.getInfo()\n",
    "        hist_list = list(hist.values())\n",
    "\n",
    "        # Plot histogram\n",
    "        plt.figure()  # This is needed to plot multiple figures in plot\n",
    "        plt.bar(hist_list[0]['bucketMeans'], hist_list[0]['histogram'], color='g')\n",
    "        plt.ylabel('Number of pixels')\n",
    "        plt.xlabel('Number of images')\n",
    "        plt.title('# of images per pixel for the period '+ \n",
    "               list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "                ' for '+ prefecture)\n",
    "        # Save figure\n",
    "        plt.savefig('../Japan_mapping/results/figures/imgavailability_period'+\n",
    "                 list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "                 prefecture +'_hist.eps', bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select prefecture polygon based on table name\n",
    "#prefecture = prefsdata['Prefecture'][0]\n",
    "# periodcol = L57_8589\n",
    "\n",
    "# Loop for each period\n",
    "for periodcol in periods_dict.values():\n",
    "\n",
    "    # Loop for each prefecture\n",
    "    for prefecture in prefsdata['Prefecture']:\n",
    "\n",
    "        # Extract prefecture polygon\n",
    "        prefpolygon = prefs \\\n",
    "            .filter(ee.Filter.eq(\"NAME_1\", prefecture)) \n",
    "\n",
    "        #Map.addLayer(Ibaraki, {}, 'Ibaraki')\n",
    "        #print(prefecture)\n",
    "        #print(list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)])\n",
    "\n",
    "prefecture = prefsdata['Prefecture'][0]\n",
    "periodcol = L57_8589\n",
    "\n",
    "# Extract flooding periods\n",
    "# Flooding start day is considered the first day of transplanting (TS)\n",
    "flood_start = int(prefsdata['startdate'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]])\n",
    "# Flooding last day is considered the end day of transplanting + 30 days\n",
    "flood_end = int(prefsdata['enddate'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]] + 30)\n",
    "\n",
    "# Extracting DOY for temperature data\n",
    "TGS0S = int(tempdata['TGS-0-S'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]])\n",
    "TGS0E = int(tempdata['TGS-0-E'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]]) \n",
    "TGS5S = int(tempdata['TGS-5-S'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]]) \n",
    "TGS5E = int(tempdata['TGS-5-E'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]]) \n",
    "TGS10S = int(tempdata['TGS-10-S'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]]) \n",
    "TGS10E = int(tempdata['TGS-10-E'][prefsdata[prefsdata['Prefecture']==prefecture].index[0]])\n",
    "\n",
    "# List for each period (year period and irrigation period)\n",
    "# Subset to flooding period\n",
    "periodcol_flood = periodcol.filter(ee.Filter.gte('DOY', flood_start)).filter(ee.Filter.lte('DOY', flood_end))\n",
    "\n",
    "# Subset to non-flooding period\n",
    "periodcol_noflood1 = periodcol.filter(ee.Filter.lte('DOY', flood_start))\n",
    "periodcol_noflood2 = periodcol.filter(ee.Filter.gte('DOY', flood_end))\n",
    "periodcol_noflood = periodcol_noflood1.merge(periodcol_noflood2)\n",
    "\n",
    "# Print number of images per season\n",
    "print('Number of Images during non-flooding season for', prefecture, \n",
    "      'for the period', list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)], \n",
    "      str(periodcol_flood.size().getInfo())+'\\n')\n",
    "print('Number of Images during non-flooding season for', prefecture, \n",
    "      'for the period', list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)],\n",
    "      str(periodcol_noflood.size().getInfo())+'\\n')\n",
    "\n",
    "# Add number of images per season and period into table\n",
    "d = {'Prefecture': [prefecture], \n",
    "     'Lustrum': [list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]], \n",
    "     'AvImFlooding': [periodcol_flood.size().getInfo()], \n",
    "     'AvImNonFlooding': [periodcol_noflood.size().getInfo()]} \n",
    "availabilitytable_pref = pd.DataFrame(data=d)\n",
    "\n",
    "# Add prefecture to table\n",
    "availabilitytable = availabilitytable.append(availabilitytable_pref)\n",
    "\n",
    "#print(availabilitytable_pref)\n",
    "#print(availabilitytable)\n",
    "\n",
    "\n",
    "# Create Histogram of availability per pixel and map!\n",
    "freq = periodcol_flood.reduce(ee.Reducer.countDistinct())\n",
    "\n",
    "# Histogram of number of images per pixel\n",
    "histogram = freq.reduceRegion(**{\n",
    "  'reducer': ee.Reducer.histogram(),\n",
    "  'geometry': prefpolygon.geometry(),\n",
    "  'scale': 30,\n",
    "  'maxPixels': 1e9\n",
    "})\n",
    "\n",
    "# Convert histogram (dictionary type) into list\n",
    "hist = histogram.getInfo()\n",
    "hist_list = list(hist.values())\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure()  # This is needed to plot multiple figures in plot\n",
    "plt.bar(hist_list[0]['bucketMeans'], hist_list[0]['histogram'], color='g')\n",
    "plt.ylabel('Number of pixels')\n",
    "plt.xlabel('Number of images')\n",
    "plt.title('# of images per pixel for the period '+ list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "         ' for ' + prefecture)\n",
    "# Save figure\n",
    "plt.savefig('../Japan_mapping/results/figures/floodimageavailability_period'+\n",
    "            list(periods_dict.keys())[list(periods_dict.values()).index(periodcol)]+\n",
    "            '_'+prefecture+'_hist.eps', bbox_inches=\"tight\")\n",
    "\n",
    "# Subset to period between first 0 and last 0 min degrees:\n",
    "L57_0S0E = periodcol.filter(ee.Filter.gte('DOY', TGS0S)).filter(ee.Filter.lte('DOY', TGS0E))\n",
    "\n",
    "# Subset to period between first 5 and last 5 min degrees:\n",
    "L57_5S5E = periodcol.filter(ee.Filter.gte('DOY', TGS5S)).filter(ee.Filter.lte('DOY', TGS5E))\n",
    "\n",
    "# Subset to period between first 0 and first 10 min degrees:\n",
    "L57_0S10S = periodcol.filter(ee.Filter.gte('DOY', TGS0S)).filter(ee.Filter.lte('DOY', TGS10S))\n",
    "\n",
    "# Subset to period between first 10 + 40days and last 10 min degrees:\n",
    "L57_10S40to5E = periodcol.filter(ee.Filter.gte('DOY', TGS10S+40)).filter(ee.Filter.lte('DOY', TGS10E))\n",
    "\n",
    "# Subset to period between last 10 and first 10 (not thermal growing season)\n",
    "L57_10E10S1 = periodcol.filter(ee.Filter.lte('DOY', TGS10S))\n",
    "L57_10E10S2 = periodcol.filter(ee.Filter.gte('DOY', TGS10E))\n",
    "L57_10E10S = L57_10E10S1.merge(L57_10E10S2)\n",
    "\n",
    "# Calculate median value for indeces for rice flooding period\n",
    "L57_flood_median = periodcol_flood.median()\n",
    "\n",
    "# Median\n",
    "L57_5S5E_median = L57_5S5E.median()\n",
    "\n",
    "# MASKS (based on Dong 2016, RSE)\n",
    "\n",
    "# Mask 1) Sparce vegetation: soil, built-up, water body, low vegetated lands. max EVI (T5S-T5E) < 0.6\n",
    "# I could use some percentile, instead of max, in order to avoid outliers\n",
    "L57_5S5E_median = L57_5S5E.median()\n",
    "L57_5S5E_max = L57_5S5E.max()\n",
    "\n",
    "mask1 = L57_5S5E_max.select('EVI').lt(0.5)\n",
    "\n",
    "# Mask 2) Natural vegetation mask: forests, natural wetlands, grass. Max EVI (T10E-T10S) > 0.4\n",
    "L57_10E10S_median = L57_10E10S.median()\n",
    "L57_10E10S_max = L57_10E10S.max()\n",
    "\n",
    "mask2 = L57_10E10S_median.select('EVI').gt(0.2)\n",
    "\n",
    "#SOLVE ISSUE: EVI GIVING VALUES ABOVE 2!\n",
    "\n",
    "# Mask 3) Forest mask (JAXA)\n",
    "# Starts from 2007\n",
    "forestdataset = ee.ImageCollection('JAXA/ALOS/PALSAR/YEARLY/FNF') \\\n",
    "    .filterDate('2017-01-01', '2017-12-31');\n",
    "    \n",
    "# Forest is the fnf = 1\n",
    "forest = forestdataset.first().select('fnf').eq(1);\n",
    "\n",
    "# Create forest mask\n",
    "mask3 = forest\n",
    "\n",
    "# MASK 3 NOT WORKING VERY WELL. ALOS MAP GETS A LOT OF FALSE POSITIVE FOREST (REDUCING RICE FIELDS)\n",
    "\n",
    "# Mask 4) Slope: larger than 3deg\n",
    "# Call SRTM elevation dataset\n",
    "elevdataset = ee.Image('JAXA/ALOS/AW3D30/V2_2');\n",
    "elevation = elevdataset.select('AVE_DSM');\n",
    "\n",
    "# Obtain \n",
    "slope = ee.Terrain.slope(elevation);\n",
    "\n",
    "# Create mask ro slope > 3 deg\n",
    "mask4 = slope.gt(3)\n",
    "\n",
    "# Method 2 for classifying based on % of images\n",
    "# Add band to each image for rice classification\n",
    "    \n",
    "periodcol_flood_masked =  periodcol_flood.map(landcovermasking)\n",
    "\n",
    "# Add rice classification layer\n",
    "periodcol_ricecol = periodcol_flood_masked.map(addRICE).select('RICE')\n",
    "\n",
    "# Now we apply the % of images threshold to classify as rice\n",
    "# Reduce the number of images with classornot pixels\n",
    "riceprop = periodcol_ricecol.map(classornot).mean()\n",
    "\n",
    "# Select % of image necessary to assing rice!\n",
    "riceclass = riceprop.gte(0.1).clip(prefpolygon)\n",
    "\n",
    "# Insert map in list\n",
    "prefmapslist.append(riceclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mosaic classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prefmapslist))\n",
    "#prefmapslist = list()\n",
    "\n",
    "periodcol = ee.ImageCollection.fromImages(prefmapslist)\n",
    "print(periodcol.size().getInfo())\n",
    "\n",
    "periodmosaic = periodcol.mosaic()\n",
    "Map3.addLayer(periodmosaic, {}, 'mosaic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vis = {\n",
    "    'bands': ['EVI'],\n",
    "    'min': 0.0,\n",
    "    'max': 2.0,\n",
    "    'gamma': 1.4,\n",
    "}\n",
    "\n",
    "Map3.addLayer(L57_10E10S_median, index_vis, 'EVI')\n",
    "Map3.addLayer(mask1, {}, 'mask1')\n",
    "Map3.addLayer(mask2, {}, 'mask2')\n",
    "Map3.addLayer(mask3, {}, 'mask3')\n",
    "Map3.addLayer(mask4, {}, 'mask4')\n",
    "#Map3.addLayer(slope, {}, 'slope')\n",
    "#Map3.addLayer(ricefinal, {}, 'finalmap')\n",
    "Map3.addLayer(riceclass, {}, 'rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join maps: Japan paddy field maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save availabilitytable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPERVIOUS AND BARREN\n",
    "# Rule for impervious and barren is LSWI < 0 \n",
    "def addIMPERV(img):    \n",
    "    imperv = img.expression(\n",
    "    \"(lswi < 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI')}).rename('IMPERV')\n",
    "    \n",
    "    return img.addBands(imperv)\n",
    "\n",
    "L57_8589_impervcol = L57_5S5E.map(addIMPERV).select('IMPERV')\n",
    "\n",
    "# Now we apply the % of images threshold to classify as rice\n",
    "def classornot(img):\n",
    "    return img.eq(1)\n",
    "\n",
    "# Reduce the number of images with classornot pixels\n",
    "impervprop = L57_8589_impervcol.map(classornot).mean()\n",
    "# Select % of image necessary to assing imperv\n",
    "impervclass = impervprop.gte(0.5)\n",
    "\n",
    "# EVERGREEN\n",
    "# Rule for evergreen is LSWI > 0 \n",
    "def addEVER(img):    \n",
    "    ever = img.expression(\n",
    "    \"(lswi > 0) ? 1\\\n",
    "    :2\",{'lswi': img.select('LSWI')}).rename('EVER')\n",
    "    \n",
    "    return img.addBands(ever)\n",
    "\n",
    "L57_8589_evercol = L57_8589.map(addEVER).select('EVER')\n",
    "\n",
    "# Now we apply the % of images threshold to classify ever\n",
    "def classornot(img):\n",
    "    return img.eq(1)\n",
    "\n",
    "# Reduce the number of images with classornot pixels\n",
    "everprop = L57_8589_evercol.map(classornot).mean()\n",
    "# Select % of image necessary to assing imperv\n",
    "everclass = everprop.gte(0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
